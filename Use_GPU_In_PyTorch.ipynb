{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFppj1o3xKMI"
   },
   "source": [
    "# Running tensors on GPUs (and making faster computations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBZs_eX2xRP7"
   },
   "source": [
    "Deep learning algorithms require a lot of numerical operations.\n",
    "\n",
    "And by default these operations are often done on a CPU (computer processing unit).\n",
    "\n",
    "However, there's another common piece of hardware called a GPU (graphics processing unit), which is often much faster at performing the specific types of operations neural networks need (matrix multiplications) than CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ-iJJHyxRMk"
   },
   "source": [
    "## 1. Getting a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj9Tc1aexRKM"
   },
   "source": [
    "There are three ways to access a GPU:\n",
    "\n",
    "* Google Colab\n",
    "* Use your own (local machine)\n",
    "* Cloud Computing (AWS , Azure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDp-TG9gxRH3"
   },
   "source": [
    "For now i prefere to use `Google Colab` because it is easy to use.\n",
    "You can use it from here https://colab.research.google.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFcvMQeExRFT"
   },
   "source": [
    "To check if you've got access to a Nvidia GPU, you can run `!nvidia-smi` where the ! (also called bang) means \"run this on the command line\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvUZlbRiyNDf",
    "outputId": "65d384de-11dd-4345-b152-5b96acc85f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l6KNls0yP4W"
   },
   "source": [
    "As you can see that it  is showing that  `command not found error` it means currently we do'nt have colab GPU access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSWfu7HryoAx"
   },
   "source": [
    "To access the colab GPU follow the below steps:\n",
    "\n",
    "1. Click on `Runtime` from the above menu.\n",
    "2. Click on `Change Runtime type`.\n",
    "3. Currently it is on CPU so click on `T4 GPU` and hit save button.\n",
    "4. It takes some seconds to connect to the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqddP6gKzTNM"
   },
   "source": [
    "Now to check again run the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZkAmQwhzTJy",
    "outputId": "04478978-565f-4003-b588-7b6309abd9b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 31 04:01:18 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaM_dMpLzTHU"
   },
   "source": [
    "Whoo!!! now we have GPU access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqEGgkAlzTE1"
   },
   "source": [
    "## 2. Getting PyTorch to run on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SFmyZ1AzTCX"
   },
   "source": [
    "Now we have GPU access! it's time to run tensors on the GPU for faster computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tJgKvhSzS_z"
   },
   "source": [
    "To do so, you can use the `torch.cuda` package.\n",
    "\n",
    "Rather than talk about it, let's try it out.\n",
    "\n",
    "You can test if PyTorch has access to a GPU using `torch.cuda.is_available()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVaRIsgi0Xwm",
    "outputId": "b16df173-323d-49e3-d0d5-630ad18e8850"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30deXT710d5U"
   },
   "source": [
    "If the above outputs `True`, PyTorch can see and use the GPU, if it outputs `False`, it can't see the GPU and in that case, you'll have to go back through the installation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LTv6gu11AwL"
   },
   "source": [
    "Now, let's say you wanted to setup your code so it ran on CPU or the GPU if it was available.\n",
    "\n",
    "That way, if you or someone decides to run your code, it'll work regardless of the computing device they're using.\n",
    "\n",
    "Let's create a device variable to store what kind of device is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dHkiq2xo1AtY",
    "outputId": "b773604e-462f-468f-c8b9-cb1e724bc742"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbQakHLf1J8b"
   },
   "source": [
    "\n",
    "If the above output \"cuda\" it means we can set all of our PyTorch code to use the available CUDA device (a GPU) and if it output \"cpu\", our PyTorch code will stick with the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q7hUMBF1PGO"
   },
   "source": [
    "You can count the number of GPUs PyTorch has access to using `torch.cuda.device_count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0cjhRHp1PC9",
    "outputId": "ff05584e-61aa-47dd-a6d9-d1f309110e90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYTV56px1PAY"
   },
   "source": [
    "## 3. Putting tensors on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQPwEPi41O-A",
    "outputId": "720f5b01-b508-4da0-cecd-77adcfe9b85b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor is running on the :cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "tensor = torch.tensor([1,2,3])\n",
    "\n",
    "print(f\"Tensor is running on the :{tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1-Y1PCF1vfy"
   },
   "source": [
    "Note: By default tensors run on the 'CPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgOuRf-Y155u",
    "outputId": "73cfbd08-a97f-4bab-9a99-6393193b7df7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor is running on:cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Move tensor to GPU (if available)\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "\n",
    "print(f\"Tensor is running on:{tensor_on_gpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxCmX07U152g"
   },
   "source": [
    "Notice the second tensor has device=`'cuda:0'`, this means it's stored on the 0th GPU available (GPUs are 0 indexed, if two GPUs were available, they'd be `'cuda:0'` and `'cuda:1'` respectively, up to `'cuda:n'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Qzf-mJ9150D"
   },
   "source": [
    "## 4. Moving tensors back to the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jJa05Uz2Xs3"
   },
   "source": [
    "What if we wanted to move the tensor back to CPU?\n",
    "\n",
    "For example, you'll want to do this if you want to interact with your tensors with NumPy (NumPy does not leverage the GPU).\n",
    "\n",
    "Let's try using the `torch.Tensor.numpy()` method on our `tensor_on_gpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "oZDSkJgt2Xpj",
    "outputId": "c3cf80ec-38be-4b7b-bc00-fd9339e477a1"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-53175578f49e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If tensor is on GPU, can't transform it to NumPy (this will error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtensor_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# If tensor is on GPU, can't transform it to NumPy (this will error)\n",
    "tensor_on_gpu.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdG_lmLX2XnB"
   },
   "source": [
    "Instead, to get a tensor back to CPU and usable with NumPy we can use Tensor.cpu().\n",
    "\n",
    "This copies the tensor to CPU memory so it's usable with CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAmjwA_T27gx",
    "outputId": "a9a67436-40e4-4eeb-86cf-20c5c9293768"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead, copy the tensor back to cpu\n",
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
